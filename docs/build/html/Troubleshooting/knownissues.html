<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Known Issues &mdash; Omnia 1.4 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Frequently Asked Questions" href="FAQ.html" />
    <link rel="prev" title="Troubleshooting" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/omnia-logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                1.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Overview/index.html">Omnia: Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../GettingStarted/index.html">Getting Started With Omnia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../RunningControlPlane/index.html">Running Control Plane</a></li>
<li class="toctree-l1"><a class="reference internal" href="../RunningOmnia/index.html">Running Omnia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../EnablingOptionalFeatures/index.html">Enabling Optional Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Telemetry_Visualization/index.html">Telemetry And Visualization</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Troubleshooting</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Known Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="FAQ.html">Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="troubleshootingguide.html">Troubleshooting Guide</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bestpractices.html">Best Practices</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Omnia</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Troubleshooting</a> &raquo;</li>
      <li>Known Issues</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/Troubleshooting/knownissues.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="known-issues">
<h1>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading"></a></h1>
<p>What to do when hosts do not show on the AWX UI?</p>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Verify if the provisioned_hosts.yml file is present in the omnia/control_plane/roles/collect_node_info/files/ folder.</p></li>
<li><p>Verify whether the hosts are listed in the provisioned_hosts.yml file.</p></li>
<li><p>If hosts are not listed, then servers are not PXE booted yet.</p></li>
<li><p>If hosts are listed, then an IP address has been assigned to them by DHCP. However, hosts are not displayed on the AWX UI as the PXE boot is still in process or is not initiated.</p></li>
<li><p>Check for the reachable and unreachable hosts using the provision_report.yml tool present in the omnia/control_plane/tools folder. To run provision_report.yml, in the omnia/control_plane/ directory, run playbook -i roles/collect_node_info/files/provisioned_hosts.yml tools/provision_report.yml.</p></li>
</ul>
<p>Why does the task ‘nfs_client: Mount NFS client’ fail with <code class="docutils literal notranslate"><span class="pre">No</span> <span class="pre">route</span> <span class="pre">to</span> <span class="pre">host</span></code>?</p>
<p><strong>Potential Cause</strong>:</p>
<ul class="simple">
<li><p>There’s a mismatch in the share path listed in <code class="docutils literal notranslate"><span class="pre">/etc/exports</span></code> and in <code class="docutils literal notranslate"><span class="pre">omnia_config.yml</span></code> under <code class="docutils literal notranslate"><span class="pre">nfs_client_params</span></code>.</p></li>
</ul>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Ensure that the input paths are a perfect match down to the character to avoid any errors.</p></li>
</ul>
<p>Why does the task ‘control plane security: Authenticate as admin’ fail?</p>
<p><strong>Potential Cause</strong>:
The required services are not running on the control plane. Verify the service status using:</p>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">sssd-kcm.socket</span></code>
<code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">sssd.service</span></code></p>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Restart the services using:</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">start</span> <span class="pre">sssd-kcm.socket</span></code>
<code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">start</span> <span class="pre">sssd.service</span></code></p>
<ul class="simple">
<li><p>Re-run <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> using the tags <code class="docutils literal notranslate"><span class="pre">init</span></code> and <code class="docutils literal notranslate"><span class="pre">security</span></code>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Ansible-playbook</span> <span class="pre">control_plane.yml</span> <span class="pre">–tags</span> <span class="pre">init,security</span></code></p>
<p>Why does the task ‘Gather facts from all the nodes’ stuck when re-running <code class="docutils literal notranslate"><span class="pre">omnia.yml</span></code>?</p>
<p><strong>Potential Cause</strong>: Corrupted entries in the <code class="docutils literal notranslate"><span class="pre">/root/.ansible/cp/</span></code> folder. For more information on this issue, <a class="reference external" href="https://github.com/ansible/ansible/issues/17349">check this out</a>!</p>
<p><strong>Resolution</strong>: Clear the directory <code class="docutils literal notranslate"><span class="pre">/root/.ansible/cp/</span></code> using the following commands:</p>
<p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">/root/.ansible/cp/</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">-rf</span> <span class="pre">*</span></code></p>
<p>Alternatively, run the task manually:</p>
<p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">omnia/tools</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">ansible-playbook</span> <span class="pre">gather_facts_resolution.yml</span></code></p>
<p>Why does the task ‘nfs_client: Mount NFS client’ fail with <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">mount</span> <span class="pre">NFS</span> <span class="pre">client.</span> <span class="pre">Make</span> <span class="pre">sure</span> <span class="pre">NFS</span> <span class="pre">Server</span> <span class="pre">is</span> <span class="pre">running</span> <span class="pre">on</span> <span class="pre">IP</span> <span class="pre">xx.xx.xx.xx</span></code>?</p>
<p><strong>Potential Cause</strong>:</p>
<ul>
<li><p>The required services for NFS may not be running:</p>
<blockquote>
<div><ul class="simple">
<li><p>nfs</p></li>
<li><p>rpc-bind</p></li>
<li><p>mountd</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Enable the required services using <code class="docutils literal notranslate"><span class="pre">firewall-cmd</span>&#160; <span class="pre">permanent</span>&#160; <span class="pre">add-service=&lt;service</span> <span class="pre">name&gt;</span></code> and then reload the firewall using <code class="docutils literal notranslate"><span class="pre">firewall-cmd</span>&#160; <span class="pre">reload</span></code>.</p></li>
</ul>
<p>Why does <code class="docutils literal notranslate"><span class="pre">configure_device_cli</span></code> fail when <code class="docutils literal notranslate"><span class="pre">awx_web_support</span></code> is set to true in <code class="docutils literal notranslate"><span class="pre">base_vars.yml</span></code>?</p>
<p><strong>Potential Cause</strong>: CLI templates require that AWX is disabled when deployed.</p>
<p><strong>Resolution</strong>: Set <code class="docutils literal notranslate"><span class="pre">awx_web_support</span></code> to false when deploying <code class="docutils literal notranslate"><span class="pre">configure_device_cli</span></code>.</p>
<p>What to do when <code class="docutils literal notranslate"><span class="pre">omnia.yml</span></code> fails with <code class="docutils literal notranslate"><span class="pre">nfs-server.service</span> <span class="pre">might</span> <span class="pre">not</span> <span class="pre">be</span> <span class="pre">running</span> <span class="pre">on</span> <span class="pre">NFS</span> <span class="pre">Server.</span> <span class="pre">Please</span> <span class="pre">check</span> <span class="pre">or</span> <span class="pre">start</span> <span class="pre">services</span></code>?</p>
<p><strong>Potential Cause</strong>: nfs-server.service is not running on the target node.</p>
<p><strong>Resolution</strong>: Use the following commands to bring up the service:</p>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">start</span> <span class="pre">nfs-server.service</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">enable</span> <span class="pre">nfs-server.service</span></code></p>
<p>Why are service tags missing in the node inventory?</p>
<p><strong>Potential Cause</strong>: Temporary network glitches may cause a loss of information.</p>
<p><strong>Resolution</strong>: Re-run the playbook <code class="docutils literal notranslate"><span class="pre">collect_node_info.yml</span></code> to repopulate the data. Use the command <code class="docutils literal notranslate"><span class="pre">ansible-playbook</span> <span class="pre">collect_node_info.yml</span></code> to run the playbook.</p>
<p>Why do Password-less SSH tasks fail while running <code class="docutils literal notranslate"><span class="pre">collect_node_info.yml</span></code>?</p>
<p><strong>Potential Cause</strong>:</p>
<ul class="simple">
<li><p>Incorrect credentials in <code class="docutils literal notranslate"><span class="pre">login_vars.yml</span></code></p></li>
<li><p>The target device may not be server running an OS. (It may be a device on a LOM network)</p></li>
</ul>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Correct the credentials in <code class="docutils literal notranslate"><span class="pre">login_vars.yml</span></code> if the target is a server.</p></li>
<li><p>Ignore the error if your target device is a storage device, switch etc.</p></li>
</ul>
<p>Why do Kubernetes Pods show <code class="docutils literal notranslate"><span class="pre">ImagePullBack</span></code> or <code class="docutils literal notranslate"><span class="pre">ErrPullImage</span></code> errors in their status?</p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>The errors occur when the Docker pull limit is exceeded.</p></li>
</ul>
</div></blockquote>
<p><strong>Resolution</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>For <code class="docutils literal notranslate"><span class="pre">omnia.yml</span></code> and <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> : Provide the docker username and password for the Docker Hub account in the <em>omnia_config.yml</em> file and execute the playbook.</p></li>
<li><p>For HPC cluster, during <code class="docutils literal notranslate"><span class="pre">omnia.yml</span> <span class="pre">execution</span></code>, a kubernetes secret ‘dockerregcred’ will be created in default namespace and patched to service account. User needs to patch this secret in their respective namespace while deploying custom applications and use the secret as imagePullSecrets in yaml file to avoid ErrImagePull. [Click here for more info](<a class="reference external" href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/</a>)</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the playbook is already executed and the pods are in <strong>ImagePullBack</strong> state, then run <code class="docutils literal notranslate"><span class="pre">kubeadm</span> <span class="pre">reset</span> <span class="pre">-f</span></code> in all the nodes before re-executing the playbook with the docker credentials.</p>
</div>
<p>What to do after a reboot if kubectl commands return: <code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">connection</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">server</span> <span class="pre">head_node_ip:port</span> <span class="pre">was</span> <span class="pre">refused</span> <span class="pre">-</span> <span class="pre">did</span> <span class="pre">you</span> <span class="pre">specify</span> <span class="pre">the</span> <span class="pre">right</span> <span class="pre">host</span> <span class="pre">or</span> <span class="pre">port?</span></code></p>
<p>On the control plane or the manager node, run the following commands:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">swapoff</span> <span class="pre">-a</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">restart</span> <span class="pre">kubelet</span></code></p></li>
</ul>
</div></blockquote>
<p>What to do if AWX auto aborts jobs (Omnia template typically) when executed on a cluster larger than 5 nodes?</p>
<p>Use CLI to execute Omnia by default by disabling AWX (set <code class="docutils literal notranslate"><span class="pre">awx_web_support</span></code> in <code class="docutils literal notranslate"><span class="pre">base_vars.yml</span></code> to <code class="docutils literal notranslate"><span class="pre">false</span></code>).</p>
<p>How to clear up the configuration if <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> fails at the webui_awx stage:</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">webui_awx/files</span></code> directory, delete the <code class="docutils literal notranslate"><span class="pre">.tower_cli.cfg</span></code> and <code class="docutils literal notranslate"><span class="pre">.tower_vault_key</span></code> files, and then re-run <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code>.</p>
<p>Why does the task ‘Control Plane Common: Fetch the available subnets and netmasks’ fail with <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">ipv4_secondaries</span> <span class="pre">present</span></code>?</p>
<p>![img.png](../images/SharedLomError.png)</p>
<p><strong>Potential Cause</strong>: If a shared LOM environment is in use, the management network/host network NIC may only have one IP assigned to it.</p>
<p><strong>Resolution</strong>: Ensure that the NIC used for host and data connections has 2 IPs assigned to it.</p>
<p>Why does the task ‘Deploy Job Templates: Launch Device Inventory Job Template’ fail with <code class="docutils literal notranslate"><span class="pre">Monitoring</span> <span class="pre">of</span> <span class="pre">Job-</span> <span class="pre">device_inventory_job</span> <span class="pre">aborted</span> <span class="pre">due</span> <span class="pre">to</span> <span class="pre">timeout</span></code> happen?</p>
<p>![img.png](../images/DeployJobTemplateTimeoutError.png)</p>
<p><strong>Potential Cause</strong>:</p>
<p>This error is caused by design. There is a mismatch between the AWX version (20.0.0) and the AWX galaxy collection (19.4.0) version used by control plane. At the time of design (Omnia 1.2.1), these were the latest available versions of AWX/AWX galaxy collection. This will be fixed in later code releases.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This failure does not stop the execution of other tasks. Check the AWX log to verify that the script has run successfully.</p>
</div>
<p>Why does provisioning RHEL 8.3 fail on some nodes with “dasbus.error.DBusError: ‘NoneType’ object has no attribute ‘set_property’”?</p>
<p>This error is known to Red Hat and is being addressed <a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1912898">here</a>. Red Hat has offered a user intervention [here](<a class="reference external" href="https://access.redhat.com/solutions/5872751">https://access.redhat.com/solutions/5872751</a>). Omnia recommends that in the event of this failure, any OS other than RHEL 8.3.</p>
<p>Why do AWX job templates fail when <code class="docutils literal notranslate"><span class="pre">awx_web_support</span></code> is false in <code class="docutils literal notranslate"><span class="pre">base_vars.yml</span></code>?</p>
<p>As a pre-requisite to running AWX job templates, AWX should be enabled by setting <code class="docutils literal notranslate"><span class="pre">awx_web_support</span></code> to true in <code class="docutils literal notranslate"><span class="pre">base_vars.yml</span></code>.</p>
<p>Why are inventory details not updated in AWX?</p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><p>The provided device credentials may be invalid.</p>
</div></blockquote>
<p><strong>Resolution</strong> :</p>
<blockquote>
<div><p>Manually validate/update the relevant login information on the AWX settings screen</p>
</div></blockquote>
<p>Why aren’t all IPs that are available in <code class="docutils literal notranslate"><span class="pre">dhcp.leases</span></code> and <code class="docutils literal notranslate"><span class="pre">mgmt_provisioned_hosts.yml</span></code> updated in the Device Inventory Job/ iDRAC inventory during <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> execution?</p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><p>Certain IPs may not update in AWX immediately because the device may be assigned an IP previously and the DHCP lease has not expired.</p>
</div></blockquote>
<p><strong>Resolution:</strong></p>
<blockquote>
<div><p>Wait for the DHCP lease for the relevant device to expire or restart the switch/device to clear the lease.</p>
</div></blockquote>
<p>Why is the host list empty when executing <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code>?</p>
<p>Hosts that are not in DHCP mode do not get populated in the host list when <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> is run.</p>
<p>Why does the task ‘Install Packages’ fail on the NFS node with the message: <code class="docutils literal notranslate"><span class="pre">Failure</span> <span class="pre">in</span> <span class="pre">talking</span> <span class="pre">to</span> <span class="pre">yum:</span> <span class="pre">Cannot</span> <span class="pre">find</span> <span class="pre">a</span> <span class="pre">valid</span> <span class="pre">baseurl</span> <span class="pre">for</span> <span class="pre">repo:</span> <span class="pre">base/7/x86_64.</span></code></p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><p>There are connections missing on the NFS node.</p>
</div></blockquote>
<p><strong>Resolution</strong>:</p>
<blockquote>
<div><p>Ensure that there are 3 NICs being used on the NFS node:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>For provisioning the OS</p></li>
<li><p>For connecting to the internet (Management purposes)</p></li>
<li><p>For connecting to PowerVault (Data Connection)</p></li>
</ol>
</div></blockquote>
</div></blockquote>
<p>Why is the Infiniband NIC down after provisioning the server?</p>
<ol class="arabic simple">
<li><p>For servers running Rocky, enable the Infiniband NIC manually, use <code class="docutils literal notranslate"><span class="pre">ifup</span> <span class="pre">&lt;InfiniBand</span> <span class="pre">NIC&gt;</span></code>.</p></li>
<li><p>If your server is running LeapOS, ensure the following pre-requisites are met before manually bringing up the interface:</p>
<ol class="arabic simple">
<li><p>The following repositories have to be installed:</p>
<ul class="simple">
<li><p>[Leap OSS](<a class="reference external" href="http://download.opensuse.org/distribution/leap/15.3/repo/oss/">http://download.opensuse.org/distribution/leap/15.3/repo/oss/</a>)</p></li>
<li><p>[Leap Non OSS](<a class="reference external" href="http://download.opensuse.org/distribution/leap/15.3/repo/non-oss/">http://download.opensuse.org/distribution/leap/15.3/repo/non-oss/</a>)</p></li>
</ul>
</li>
<li><p>Run: <code class="docutils literal notranslate"><span class="pre">zypper</span> <span class="pre">install</span> <span class="pre">-n</span> <span class="pre">rdma-core</span> <span class="pre">librdmacm1</span> <span class="pre">libibmad5</span> <span class="pre">libibumad3</span> <span class="pre">infiniband-diags</span></code> to install IB NIC drivers.  (If the drivers do not install smoothly, reboot the server to apply the required changes)</p></li>
<li><p>Run: <code class="docutils literal notranslate"><span class="pre">service</span> <span class="pre">network</span> <span class="pre">status</span></code> to verify that <code class="docutils literal notranslate"><span class="pre">wicked.service</span></code> is running.</p></li>
<li><p>Verify that the ifcfg-&lt; InfiniBand NIC &gt; file is present in <code class="docutils literal notranslate"><span class="pre">/etc/sysconfig/network</span></code>.</p></li>
<li><p>Once all the above pre-requisites are met, bring up the interface manually using <code class="docutils literal notranslate"><span class="pre">ifup</span> <span class="pre">&lt;InfiniBand</span> <span class="pre">NIC&gt;</span></code>.</p></li>
</ol>
</li>
</ol>
<p>Alternatively, run <code class="docutils literal notranslate"><span class="pre">omnia.yml</span></code> to activate the NIC.</p>
<p>What to do if AWX jobs fail with <code class="docutils literal notranslate"><span class="pre">Error</span> <span class="pre">creating</span> <span class="pre">pod:</span> <span class="pre">container</span> <span class="pre">failed</span> <span class="pre">to</span> <span class="pre">start,</span> <span class="pre">ImagePullBackOff</span></code>?</p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><p>After running <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code>, the AWX image got deleted due to space considerations (use <code class="docutils literal notranslate"><span class="pre">df</span> <span class="pre">-h</span></code> to diagnose the issue.).</p>
</div></blockquote>
<p><strong>Resolution</strong>:</p>
<blockquote>
<div><p>Delete unnecessary files from the partition```` and then run the following commands:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">omnia/control_plane/roles/webui_awx/files</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">buildah</span> <span class="pre">bud</span> <span class="pre">-t</span> <span class="pre">custom-awx-ee</span> <span class="pre">awx_ee.yml</span></code></p></li>
</ol>
</div></blockquote>
<p>Why do pods and images appear to get deleted automatically?</p>
<p><strong>Potential Cause</strong>:</p>
<p>Lack of space in the root partition (/) causes Linux to clear files automatically (Use <code class="docutils literal notranslate"><span class="pre">df</span> <span class="pre">-h</span></code> to diagnose the issue).</p>
<blockquote>
<div><p><strong>Resolution</strong>:</p>
</div></blockquote>
<ul class="simple">
<li><p>Delete large, unused files to clear the root partition (Use the command <code class="docutils literal notranslate"><span class="pre">find</span> <span class="pre">/</span> <span class="pre">-xdev</span> <span class="pre">-size</span> <span class="pre">+5M</span> <span class="pre">|</span> <span class="pre">xargs</span> <span class="pre">ls</span> <span class="pre">-lh</span> <span class="pre">|</span> <span class="pre">sort</span> <span class="pre">-n</span> <span class="pre">-k5</span></code> to identify these files). Before running Omnia Control Plane, it is recommended to have a minimum of 50% free space in the root partition.</p></li>
<li><p>Once the partition is cleared, run <code class="docutils literal notranslate"><span class="pre">kubeadm</span> <span class="pre">reset</span> <span class="pre">-f</span></code></p></li>
<li><p>Re-run <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code></p></li>
</ul>
<p>Why does the task ‘control_plane_common: Setting Metric’ fail?</p>
<p><strong>Potential Cause</strong>:</p>
<blockquote>
<div><blockquote>
<div><p>The device name and connection name listed by the network manager in <code class="docutils literal notranslate"><span class="pre">/etc/sysconfig/network-scripts/ifcfg-&lt;nic</span> <span class="pre">name&gt;</span></code> do not match.</p>
</div></blockquote>
<p><strong>Resolution</strong>:</p>
</div></blockquote>
<ol class="arabic">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">nmcli</span> <span class="pre">connection</span></code> to list all available connections and their attributes.</p>
<blockquote>
<div><p><em>Expected Output:</em></p>
<p>![NMCLI Expected Output](../images/nmcli_output.jpg)</p>
</div></blockquote>
</li>
<li><p>For any connections that have mismatched names and device names, edit the file <code class="docutils literal notranslate"><span class="pre">/etc/sysconfig/network-scripts/ifcfg-&lt;nic</span> <span class="pre">name&gt;</span></code> using vi editor.</p></li>
</ol>
<p>Why is the error “Wait for AWX UI to be up” displayed when <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> fails?</p>
<p><strong>Potential Causes</strong>:</p>
<ol class="arabic simple">
<li><p>AWX is not accessible even after five minutes of wait time.</p></li>
<li><p><a href="#id1"><span class="problematic" id="id2">**</span></a>isMigrating ** or  <a href="#id3"><span class="problematic" id="id4">**</span></a>isInstalling ** is seen in the failure message.</p></li>
</ol>
<blockquote>
<div><p><strong>Resolution</strong>:</p>
</div></blockquote>
<p>Wait for AWX UI to be accessible at <a class="reference external" href="http://">http://</a>&lt;management-station-IP&gt;:8081, and then run the <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> file again, where  <a href="#id5"><span class="problematic" id="id6">**</span></a>management-station-IP ** is the IP address of the management node.</p>
<p>Why does Omnia Control Plane fail at Task: <code class="docutils literal notranslate"><span class="pre">control_plane_common:</span> <span class="pre">Assert</span> <span class="pre">Value</span> <span class="pre">of</span> <span class="pre">idrac_support</span> <span class="pre">if</span> <span class="pre">mngmt_network</span> <span class="pre">container</span> <span class="pre">needed</span></code>?</p>
<p>When <code class="docutils literal notranslate"><span class="pre">device_config_support</span></code> is set to true, <code class="docutils literal notranslate"><span class="pre">idrac_support</span></code> also needs to be set to true.</p>
<p>Why does the <code class="docutils literal notranslate"><span class="pre">idrac.yml</span></code> template hang during the import SCP file task on certain target nodes?</p>
<p><strong>Potential Causes</strong>:</p>
<ol class="arabic simple">
<li><p>The server hardware does not allow for auto rebooting</p></li>
<li><p>Pending jobs may be running at the time of applying the SCP configuration.</p></li>
</ol>
<p><strong>Resolution</strong>:</p>
<ol class="arabic simple">
<li><p>Login to the iDRAC console to check if the server is stuck in boot errors (F1 prompt message). If true, clear the hardware error or disable POST (PowerOn Self Test).</p></li>
<li><p>Reset iDRAC to clear the job queue (If a job is pending).</p></li>
</ol>
<p>Why is the iDRAC server not reachable after running <code class="docutils literal notranslate"><span class="pre">idrac.yml</span></code> for certain target nodes?</p>
<p><strong>Potential Causes</strong>:</p>
<ol class="arabic simple">
<li><p>The server hardware does not allow for auto rebooting</p></li>
<li><p>PXE booting is hung on the node</p></li>
</ol>
<p><strong>Resolution</strong>:</p>
<ol class="arabic simple">
<li><p>Login to the iDRAC console to check if the server is stuck in boot errors (F1 prompt message). If true, clear the hardware error or disable POST (PowerOn Self Test).</p></li>
<li><p>Hard-reboot the server to bring up the server and verify that the boot process runs smoothly. (If it gets stuck again, disable PXE and try provisioning the server via iDRAC.)</p></li>
</ol>
<p>What to do if the nodes in a Kubernetes cluster reboot:</p>
<p>Wait for 15 minutes after the Kubernetes cluster reboots. Next, verify the status of the cluster using the following commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">nodes</span></code> on the manager node to get the real-time k8s cluster status.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span>&#160; <span class="pre">all-namespaces</span></code> on the manager node to check which the pods are in the <strong>Running</strong> state.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">cluster-info</span></code> on the manager node to verify that both the k8s master and kubeDNS are in the <strong>Running</strong> state.</p></li>
</ul>
<p>What to do when the Kubernetes services are not in the  <a href="#id7"><span class="problematic" id="id8">**</span></a>Running **  state:</p>
<ol class="arabic simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span>&#160; <span class="pre">all-namespaces</span></code> to verify that all pods are in the <strong>Running</strong> state.</p></li>
<li><p>If the pods are not in the <strong>Running</strong> state, delete the pods using the command:<code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">delete</span> <span class="pre">pods</span> <span class="pre">&lt;name</span> <span class="pre">of</span> <span class="pre">pod&gt;</span></code></p></li>
<li><p>Run the corresponding playbook that was used to install Kubernetes: <code class="docutils literal notranslate"><span class="pre">omnia.yml</span></code>, <code class="docutils literal notranslate"><span class="pre">jupyterhub.yml</span></code>, or <code class="docutils literal notranslate"><span class="pre">kubeflow.yml</span></code>.</p></li>
</ol>
<p>What to do when the JupyterHub or Prometheus UI is not accessible:</p>
<p>Run the command <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span>&#160; <span class="pre">namespace</span> <span class="pre">default</span></code> to ensure <strong>nfs-client</strong> pod and all Prometheus server pods are in the <strong>Running</strong> state.</p>
<p>While configuring Cobbler, why does the <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> fail during the Run import command?</p>
<p><strong>Cause</strong>:</p>
<ul>
<li><p>The mounted .iso file is corrupt.</p>
<p><strong>Resolution</strong>:</p>
</li>
</ul>
<ol class="arabic simple">
<li><p>Go to  <a href="#id9"><span class="problematic" id="id10">**</span></a>var <a href="#id11"><span class="problematic" id="id12">**</span></a>-&gt; <a href="#id13"><span class="problematic" id="id14">**</span></a>log <a href="#id15"><span class="problematic" id="id16">**</span></a>-&gt; <a href="#id17"><span class="problematic" id="id18">**</span></a>cobbler <a href="#id19"><span class="problematic" id="id20">**</span></a>-&gt; <a href="#id21"><span class="problematic" id="id22">**</span></a>cobbler.log ** to view the error.</p></li>
<li><p>If the error message is <strong>repo verification failed</strong>, the .iso file is not mounted properly.</p></li>
<li><p>Verify that the downloaded .iso file is valid and correct.</p></li>
<li><p>Delete the Cobbler container using <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">rm</span> <span class="pre">-f</span> <span class="pre">cobbler</span></code> and rerun <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code>.</p></li>
</ol>
<p>Why does PXE boot fail with tftp timeout or service timeout errors?</p>
<p><strong>Potential Causes</strong>:</p>
<ul>
<li><p>RAID is configured on the server.</p></li>
<li><p>Two or more servers in the same network have Cobbler services running.</p></li>
<li><p>The target compute node does not have a configured PXE device with an active NIC.</p>
<p><strong>Resolution</strong>:</p>
</li>
</ul>
<ol class="arabic simple">
<li><p>Create a Non-RAID or virtual disk on the server.</p></li>
<li><p>Check if other systems except for the management node have cobblerd running. If yes, then stop the Cobbler container using the following commands: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">rm</span> <span class="pre">-f</span> <span class="pre">cobbler</span></code> and <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">image</span> <span class="pre">rm</span> <span class="pre">-f</span> <span class="pre">cobbler</span></code>.</p></li>
<li><p>On the server, go to <code class="docutils literal notranslate"><span class="pre">BIOS</span> <span class="pre">Setup</span> <span class="pre">-&gt;</span> <span class="pre">Network</span> <span class="pre">Settings</span> <span class="pre">-&gt;</span> <span class="pre">PXE</span> <span class="pre">Device</span></code>. For each listed device (typically 4), configure an active NIC under <code class="docutils literal notranslate"><span class="pre">PXE</span> <span class="pre">device</span> <span class="pre">settings</span></code></p></li>
</ol>
<p>What to do when Slurm services do not start automatically after the cluster reboots:</p>
<ul class="simple">
<li><p>Manually restart the slurmd services on the manager node by running the following commands:</p></li>
</ul>
<div class="line-block">
<div class="line">systemctl restart slurmdbd</div>
<div class="line">systemctl restart slurmctld</div>
<div class="line">systemctl restart prometheus-slurm-exporter</div>
</div>
<ul class="simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">slurmd</span></code> to manually restart the following service on all the compute nodes.</p></li>
</ul>
<p>Why do Slurm services fail?</p>
<p><strong>Potential Cause</strong>: The <code class="docutils literal notranslate"><span class="pre">slurm.conf</span></code> is not configured properly.</p>
<p>Recommended Actions:</p>
<ol class="arabic simple">
<li><p>Run the following commands:</p></li>
</ol>
<div class="line-block">
<div class="line">slurmdbd -Dvvv</div>
<div class="line">slurmctld -Dvvv</div>
</div>
<ol class="arabic simple" start="2">
<li><p>Refer the <code class="docutils literal notranslate"><span class="pre">/var/lib/log/slurmctld.log</span></code> file for more information.</p></li>
</ol>
<p>## What causes the “Ports are Unavailable” error?</p>
<p>Cause: Slurm database connection fails.</p>
<p>Recommended Actions:</p>
<ol class="arabic simple">
<li><p>Run the following commands:</p></li>
</ol>
<div class="line-block">
<div class="line">slurmdbd -Dvvv</div>
</div>
<div class="line-block">
<div class="line">slurmctld -Dvvv</div>
</div>
<ol class="arabic simple" start="2">
<li><p>Refer the <code class="docutils literal notranslate"><span class="pre">/var/lib/log/slurmctld.log</span></code> file.</p></li>
<li><p>Check the output of <code class="docutils literal notranslate"><span class="pre">netstat</span> <span class="pre">-antp</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">LISTEN</span></code> for  PIDs in the listening state.</p></li>
<li><p>If PIDs are in the <strong>Listening</strong> state, kill the processes of that specific port.</p></li>
<li><p>Restart all Slurm services:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">slurmctl</span> <span class="pre">restart</span> <span class="pre">slurmctld</span></code> on manager node</p>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">restart</span> <span class="pre">slurmdbd</span></code> on manager node</p>
<p><code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">restart</span> <span class="pre">slurmd</span></code> on compute node</p>
<p>Why do Kubernetes Pods stop communicating with the servers when the DNS servers are not responding?</p>
<p><strong>Potential Cause</strong>: The host network is faulty causing DNS to be unresponsive</p>
<p><strong>Resolution</strong>:</p>
<ol class="arabic simple">
<li><p>In your Kubernetes cluster, run <code class="docutils literal notranslate"><span class="pre">kubeadm</span> <span class="pre">reset</span> <span class="pre">-f</span></code> on all the nodes.</p></li>
<li><p>On the management node, edit the <code class="docutils literal notranslate"><span class="pre">omnia_config.yml</span></code> file to change the Kubernetes Pod Network CIDR. The suggested IP range is 192.168.0.0/16. Ensure that the IP provided is not in use on your host network.</p></li>
<li><p>Execute omnia.yml and skip slurm <code class="docutils literal notranslate"><span class="pre">ansible-playbook</span> <span class="pre">omnia.yml</span>&#160; <span class="pre">skip-tags</span> <span class="pre">slurm</span></code></p></li>
</ol>
<p>Why does pulling images to create the Kubeflow timeout causing the ‘Apply Kubeflow Configuration’ task to fail?</p>
<p><strong>Potential Cause</strong>: Unstable or slow Internet connectivity.</p>
<p><strong>Resolution</strong>:</p>
<ol class="arabic simple">
<li><p>Complete the PXE booting/format the OS on the manager and compute nodes.</p></li>
<li><p>In the omnia_config.yml file, change the k8s_cni variable value from <code class="docutils literal notranslate"><span class="pre">calico</span></code> to <code class="docutils literal notranslate"><span class="pre">flannel</span></code>.</p></li>
<li><p>Run the Kubernetes and Kubeflow playbooks.</p></li>
</ol>
<p>What to do if jobs hang in ‘pending’ state on the AWX UI:</p>
<p>Run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">rollout</span> <span class="pre">restart</span> <span class="pre">deployment</span> <span class="pre">awx</span> <span class="pre">-n</span> <span class="pre">awx</span></code> from the control plane and try to re-run the job.</p>
<p>If the above solution <strong>doesn’t work</strong>,</p>
<ol class="arabic simple">
<li><p>Delete all the inventories, groups and organization from AWX UI.</p></li>
<li><p>Delete the folder: <code class="docutils literal notranslate"><span class="pre">/var/nfs_awx</span></code>.</p></li>
<li><p>Delete the file: <code class="docutils literal notranslate"><span class="pre">omnia/control_plane/roles/webui_awx/files/.tower_cli.cfg</span></code>.</p></li>
<li><p>Re-run <em>control_plane.yml</em>.</p></li>
</ol>
<p>Why is my NFS mount not visible on the client?</p>
<p><strong>Potential Cause</strong>: The directory being used by the client as a mount point is already in use by a different NFS export.</p>
<p><strong>Resolution</strong>: Verify that the directory being used as a mount point is empty by using <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">&lt;client</span> <span class="pre">share</span> <span class="pre">path&gt;</span> <span class="pre">|</span> <span class="pre">ls</span></code> or <code class="docutils literal notranslate"><span class="pre">mount</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">&lt;client</span> <span class="pre">share</span> <span class="pre">path&gt;</span></code>. If empty, re-run the playbook.</p>
<p>![](../images/omnia_NFS_mount_fcfs.png)</p>
<p>What to do after a control plane reboot?</p>
<ol class="arabic simple">
<li><p>Once the control plane reboots, wait for 10-15 minutes to allow all k8s pods and services to come up. This can be verified using:</p></li>
</ol>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span>&#160; <span class="pre">all-namespaces</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>If the pods do not come up, check <code class="docutils literal notranslate"><span class="pre">/var/log/omnia/startup_omnia/startup_omnia_yyyy-mm-dd-HHMMSS.log</span></code> for more information.</p></li>
<li><p>Cobbler profiles are not persistent across reboots. The latest profile will be available post-reboot based on the values of <code class="docutils literal notranslate"><span class="pre">provision_os</span></code> and <code class="docutils literal notranslate"><span class="pre">iso_file_path</span></code> in <code class="docutils literal notranslate"><span class="pre">base_vars.yml</span></code>. Re-run <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> with different values for <code class="docutils literal notranslate"><span class="pre">provision_os</span></code> and <code class="docutils literal notranslate"><span class="pre">iso_file_path</span></code> to restore the profiles.</p></li>
<li><p>Devices that have had their IP assigned dynamically via DHCP may get assigned new IPs. This in turn can cause duplicate entries for the same device on AWX. Clusters may also show inconsistency and ambiguity.</p></li>
</ol>
<p>Why is permission denied when executing the <code class="docutils literal notranslate"><span class="pre">idrac.yml</span></code> file or other .yml files from AWX?</p>
<p><strong>Potential Cause</strong>: The “PermissionError: [Errno 13] Permission denied” error is displayed if you have used the ansible-vault decrypt or encrypt commands.</p>
<p><strong>Resolution</strong>:</p>
<ul class="simple">
<li><p>Update permissions on the relevant .yml using <code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">664</span> <span class="pre">&lt;filename&gt;.yml</span></code></p></li>
</ul>
<p>It is recommended that the ansible-vault view or edit commands are used and not the ansible-vault decrypt or encrypt commands.</p>
<p>What to do if the network CIDR entry of iDRAC IP in /etc/exports file is missing:</p>
<ul class="simple">
<li><p>Add an additional network CIDR range of iDRAC IPs in the <em>/etc/exports</em> file if the iDRAC IP is not in the management network range provided in base_vars.yml.</p></li>
</ul>
<p>What to do if a custom ISO file is not present on the device:</p>
<ul class="simple">
<li><p>Re-run the <em>control_plane.yml</em> file.</p></li>
</ul>
<p>What to do if the <em>management_station_ip.txt</em> file under <em>provision_idrac/files</em> folder is missing:</p>
<ul class="simple">
<li><p>Re-run the <em>control_plane.yml</em> file.</p></li>
</ul>
<p>The provisioning of PowerEdge servers failed. How do I clean up before starting over?</p>
<ol class="arabic simple">
<li><p>Delete the respective iDRAC IP addresses from the <em>provisioned_idrac_inventory</em> on the AWX UI or delete the <em>provisioned_idrac_inventory</em> to delete the iDRAC IP addresses of all the servers in the cluster.</p></li>
<li><p>Launch the iDRAC template from the AWX UI.</p></li>
</ol>
<p>What to do if PowerVault throws the error: <code class="docutils literal notranslate"><span class="pre">Error:</span> <span class="pre">The</span> <span class="pre">specified</span> <span class="pre">disk</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">available.</span> <span class="pre">-</span> <span class="pre">Unavailable</span> <span class="pre">disk</span> <span class="pre">(0.x)</span> <span class="pre">in</span> <span class="pre">disk</span> <span class="pre">range</span> <span class="pre">'0.x-x'</span></code>:</p>
<ol class="arabic simple">
<li><p>Verify that the disk in question is not part of any pool: <code class="docutils literal notranslate"><span class="pre">show</span> <span class="pre">disks</span></code></p></li>
<li><p>If the disk is part of a pool, remove it and try again.</p></li>
</ol>
<p>Why does PowerVault throw the error: <code class="docutils literal notranslate"><span class="pre">You</span> <span class="pre">cannot</span> <span class="pre">create</span> <span class="pre">a</span> <span class="pre">linear</span> <span class="pre">disk</span> <span class="pre">group</span> <span class="pre">when</span> <span class="pre">a</span> <span class="pre">virtual</span> <span class="pre">disk</span> <span class="pre">group</span> <span class="pre">exists</span> <span class="pre">on</span> <span class="pre">the</span> <span class="pre">system.</span></code>?</p>
<p>At any given time only one type of disk group can be created on the system. That is, all disk groups on the system have to exclusively be linear or virtual. To fix the issue, either delete the existing disk group or change the type of pool you are creating.</p>
<p>What to do when iDRAC template execution throws a warning regarding older firmware versions:</p>
<p><strong>Potential Cause</strong>: Older firmware version in PowerEdge servers. Omnia supports only iDRAC 8 based Dell EMC PowerEdge Servers with firmware versions 2.75.75.75 and above and iDRAC 9 based Dell EMC PowerEdge Servers with Firmware versions 4.40.40.00 and above.</p>
<ol class="arabic simple">
<li><p>Update iDRAC firmware version in PowerEdge servers manually to the supported version.</p></li>
<li><p>Re-run idrac_template.</p></li>
</ol>
<p>Why does the ‘Initialize Kubeadm’ task fail with ‘nnode.Registration.name: Invalid value: &quot;&lt;Host name&gt;&quot;’?</p>
<p><strong>Potential Cause</strong>: The control_plane playbook does not support hostnames with an underscore in it such as ‘mgmt_station’.</p>
<p>As defined in RFC 822, the only legal characters are the following:
1. Alphanumeric (a-z and 0-9): Both uppercase and lowercase letters are acceptable, and the hostname is case-insensitive. In other words, dvader.empire.gov is identical to DVADER.EMPIRE.GOV and Dvader.Empire.Gov.</p>
<ol class="arabic simple" start="2">
<li><p>Hyphen (-): Neither the first nor the last character in a hostname field should be a hyphen.</p></li>
<li><p>Period (.): The period should be used only to delimit fields in a hostname (e.g., dvader.empire.gov)</p></li>
</ol>
<p>What to do when JupyterHub pods are in ‘ImagePullBackOff’ or ‘ErrImagePull’ status after executing jupyterhub.yml:</p>
<p><strong>Potential Cause</strong>: Your Docker pull limit has been exceeded. For more information, click [here](<a class="reference external" href="https://www.docker.com/increase-rate-limits">https://www.docker.com/increase-rate-limits</a>)
1. Delete Jupyterhub deployment by executing the following command in manager node: <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">delete</span> <span class="pre">jupyterhub</span> <span class="pre">-n</span> <span class="pre">jupyterhub</span></code>
2. Re-execute jupyterhub.yml after 8-9 hours.</p>
<p>What to do when Kubeflow pods are in ‘ImagePullBackOff’ or ‘ErrImagePull’ status after executing kubeflow.yml:</p>
<p><strong>Potential Cause</strong>: Your Docker pull limit has been exceeded. For more information, click [here](<a class="reference external" href="https://www.docker.com/increase-rate-limits">https://www.docker.com/increase-rate-limits</a>)
1. Delete Kubeflow deployment by executing the following command in manager node: <code class="docutils literal notranslate"><span class="pre">kfctl</span> <span class="pre">delete</span> <span class="pre">-V</span> <span class="pre">-f</span> <span class="pre">/root/k8s/omnia-kubeflow/kfctl_k8s_istio.v1.0.2.yaml</span></code>
2. Re-execute kubeflow.yml after 8-9 hours</p>
<p>Why do Firmware Updates fail for some components with Omnia?</p>
<p>Due to the latest <code class="docutils literal notranslate"><span class="pre">catalog.xml</span></code> file, Firmware updates may fail for certain components. Omnia execution doesn’t get interrupted but an error gets logged on AWX. For now, please download those individual updates manually.</p>
<p>Why does the Task [network_ib : Authentication failure response] fail with the message ‘Status code was -1 and not [302]: Request failed: &lt;urlopen error [Errno 111] Connection refused&gt;’ on Infiniband Switches when running <code class="docutils literal notranslate"><span class="pre">infiniband.yml</span></code>?</p>
<p>To configure a new Infiniband Switch, it is required that HTTP and JSON gateway be enabled. To verify that they are enabled, run:</p>
<p><code class="docutils literal notranslate"><span class="pre">show</span> <span class="pre">web</span></code> (To check if HTTP is enabled)</p>
<p><code class="docutils literal notranslate"><span class="pre">show</span> <span class="pre">json-gw</span></code> (To check if JSON Gateway is enabled)</p>
<p>To correct the issue, run:</p>
<p><code class="docutils literal notranslate"><span class="pre">web</span> <span class="pre">http</span> <span class="pre">enable</span></code> (To enable the HTTP gateway)</p>
<p><code class="docutils literal notranslate"><span class="pre">json-gw</span> <span class="pre">enable</span></code> (To enable the JSON gateway)</p>
<dl class="simple">
<dt>Why does the <code class="docutils literal notranslate"><span class="pre">BeeGFS-client</span></code> service fail?</dt><dd><ul class="simple">
<li></li>
</ul>
</dd>
</dl>
<p><strong>Potential Causes</strong>:</p>
<ol class="arabic simple">
<li><p>SELINUX may be enabled. (use <code class="docutils literal notranslate"><span class="pre">sestatus</span></code> to diagnose the issue)</p></li>
<li><p>Ports 8008, 8003, 8004, 8005 and 8006 may be closed. (use <code class="docutils literal notranslate"><span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">beegfs-mgmtd,</span> <span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">beegfs-meta,</span> <span class="pre">systemctl</span> <span class="pre">status</span> <span class="pre">beegfs-storage</span></code> to diagnose the issue)</p></li>
<li><p>The BeeGFS set up may be incompatible with Red Hat.</p></li>
</ol>
<p><strong>Resolution</strong>:</p>
<ol class="arabic simple">
<li><p>If SeLinux is enabled, update the file <code class="docutils literal notranslate"><span class="pre">/etc/sysconfig/selinux</span></code> and reboot the server.</p></li>
<li><p>Open all ports required by BeeGFS: 8008, 8003, 8004, 8005 and 8006</p></li>
<li><p>Check the [support matrix for Red Hat or Rocky](../Support_Matrix/Software/Operating_Systems) to verify your set-up.</p></li>
<li><p>For further insight into the issue, check out <code class="docutils literal notranslate"><span class="pre">/var/log/beegfs-client.log</span></code></p></li>
</ol>
<p>Why are the PXE device settings not configured by Omnia on some servers?</p>
<p>While the NIC qualifies as active, it may not qualify as a PXE device NIC (It may be a mellanox NIC). In such a situation, Omnia assumes that PXE device settings are already configured and proceeds to attempt a PXE boot.
If this is not the case, manually configure a PXE device NIC and re-run <code class="docutils literal notranslate"><span class="pre">idrac.yml</span></code> to proceed.</p>
<p>What to do when <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code> fail with ‘Error: kinit: Connection refused while getting default ccache’ while completing the control plane security role?</p>
<ol class="arabic simple">
<li><p>Start the sssd-kcm.socket: <code class="docutils literal notranslate"><span class="pre">systemcl</span> <span class="pre">start</span> <span class="pre">sssd-kcm.socket</span></code></p></li>
<li><p>Re-run <code class="docutils literal notranslate"><span class="pre">control_plane.yml</span></code></p></li>
</ol>
<p>Why does installing FreeIPA fail on Red Hat servers?</p>
<p>![](../images/FreeIPA_RHEL_Error.png)</p>
<p><strong>Potential Causes</strong>: Required repositories may not be enabled by your red hat subscription.</p>
<p><strong>Resolution</strong>: Enable all required repositories via your red hat subscription.</p>
<p>Why would FreeIPA server/client installation fail?</p>
<p><strong>Potential Cause</strong>:</p>
<p>The hostnames of the manager and login nodes are not set in the correct format.</p>
<p><strong>Resolution</strong>:</p>
<p>If you have enabled the option to install the login node in the cluster, set the hostnames of the nodes in the format: <em>hostname.domainname</em>. For example, <em>manager.omnia.test</em> is a valid hostname for the login node. <strong>Note</strong>: To find the cause for the failure of the FreeIPA server and client installation, see <em>ipaserver-install.log</em> in the manager node or <em>/var/log/ipaclient-install.log</em> in the login node.</p>
<p>Why does FreeIPA installation fail on the control plane when the public NIC provided is static?</p>
<p><strong>Potential Cause</strong>: The network config file for the public NIC on the control plane does not define any DNS entries.</p>
<p><strong>Resolution</strong>: Ensure the fields <code class="docutils literal notranslate"><span class="pre">DNS1</span></code> and <code class="docutils literal notranslate"><span class="pre">DNS2</span></code> are updated appropriately in the file <code class="docutils literal notranslate"><span class="pre">/etc/sysconfig/network-scripts/ifcfg-&lt;NIC</span> <span class="pre">name&gt;</span></code>.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Troubleshooting" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="FAQ.html" class="btn btn-neutral float-right" title="Frequently Asked Questions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Cassey Goveas.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>