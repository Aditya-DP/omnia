---
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: gaudi-llm-ds-ft
  namespace: workloads
spec:
  slotsPerWorker: 8
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
            - image: vault.habana.ai/gaudi-docker/1.17.1/ubuntu22.04/habanalabs/pytorch-installer-2.3.1:latest
              name: gaudi-llm-ds-ft-launcher
              env:
                - name: HF_HOME
                  value: /storage/models
                - name: http_proxy
                  value: ""
                - name: https_proxy
                  value: ""
                - name: HTTP_PROXY
                  value: ""
                - name: HTTPS_PROXY
                  value: ""
                - name: no_proxy
                  value: ""
                - name: LLM_MODEL
                  value: meta-llama/Meta-Llama-3-70B-Instruct
                - name: NUM_HPU
                  value: "8"
                - name: NUM_EPOCHS
                  value: "3"
                - name: HUGGING_FACE_HUB_TOKEN
                  value: ""
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  HOSTSFILE=$OMPI_MCA_orte_default_hostfile;
                  MASTER_ADDR="$(head -n 1 $HOSTSFILE | sed -n s/[[:space:]]slots.*//p)";
                  SETUP_CMD="git clone  https://github.com/huggingface/optimum-habana /optimum-habana";
                  export no_proxy=$no_proxy,$KUBERNETES_SERVICE_HOST;
                  NUM_NODES=$(wc -l < $HOSTSFILE);
                  N_CARDS=$((NUM_NODES*NUM_HPU));

                  git clone  https://github.com/huggingface/optimum-habana /optimum-habana;
                  cd /optimum-habana;
                  git checkout v1.12.0;
                  sed -i '194s|deepspeed|deepspeed --force_multi|' optimum/habana/distributed/distributed_runner.py;
                  pip install .;
                  pip install -r examples/language-modeling/requirements.txt;
                  pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.16.2;
                  mpirun --npernode 1 \
                    --tag-output \
                    --allow-run-as-root \
                    --prefix $MPI_ROOT \
                    -x http_proxy=$http_proxy \
                    -x https_proxy=$https_proxy \
                    -x no_proxy=$no_proxy \
                    -x LLM_MODEL=$LLM_MODEL \
                    -x HF_HOME=$HF_HOME \
                    -x NUM_HPU=$NUM_HPU \
                    -x HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \
                    bash -i -c '
                    git clone  https://github.com/huggingface/optimum-habana /optimum-habana
                    cd /optimum-habana
                    git checkout v1.12.0
                    hf_home_var="os.environ[\"HF_HOME\"] = \"${HF_HOME}\""
                    token_var="os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"${HUGGING_FACE_HUB_TOKEN}\""
                    https_var="os.environ[\"https_proxy\"] = \"${https_proxy}\""
                    http_var="os.environ[\"http_proxy\"] = \"${http_proxy}\""
                    no_proxy_var="os.environ[\"no_proxy\"] = \"${no_proxy}\""
                    sed -i "59i\\${https_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "60i\\${http_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "61i\\${hf_home_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "62i\\${token_var}" examples/language-modeling/run_lora_clm.py
                    sed -i "63i\\${no_proxy_var}" examples/language-modeling/run_lora_clm.py

                    pip install .
                    pip install -r examples/language-modeling/requirements.txt
                    pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.16.2
                    ';

                    cd /optimum-habana/examples/language-modeling/;
                    python3 ../gaudi_spawn.py --hostfile=$HOSTSFILE --use_deepspeed --world_size $N_CARDS  run_lora_clm.py \
                    --model_name_or_path $LLM_MODEL \
                    --deepspeed llama2_ds_zero3_config.json \
                    --dataset_name tatsu-lab/alpaca \
                    --bf16 True \
                    --output_dir ./lora_out \
                    --num_train_epochs 2 \
                    --max_seq_len 2048 \
                    --per_device_train_batch_size 10 \
                    --per_device_eval_batch_size 10 \
                    --gradient_checkpointing \
                    --evaluation_strategy epoch \
                    --eval_delay 2 \
                    --save_strategy no \
                    --learning_rate 0.0018 \
                    --warmup_ratio 0.03 \
                    --lr_scheduler_type "cosine" \
                    --logging_steps 1 \
                    --dataset_concatenation \
                    --attn_softmax_bf16 True \
                    --do_train \
                    --do_eval \
                    --use_habana \
                    --use_lazy_mode \
                    --pipelining_fwd_bwd \
                    --throughput_warmup_steps 3 \
                    --lora_rank 4 \
                    --lora_target_modules "q_proj" "v_proj" "k_proj" "o_proj" \
                    --validation_split_percentage 4 \
                    --use_flash_attention True \
                    --flash_attention_causal_mask True
              volumeMounts:
                - name: datasets
                  mountPath: /storage
          volumes:
            - name: datasets
              persistentVolumeClaim:
                claimName: shared-model
                readOnly: false
    Worker:
      replicas: 1
      template:
        spec:
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.17.1/ubuntu22.04/habanalabs/pytorch-installer-2.3.1:latest
              name: gaudi-llm-ds-ft-worker
              resources:
                limits:
                  habana.ai/gaudi: 8
                  memory: 400Gi
                  hugepages-2Mi: 95000Mi
                requests:
                  habana.ai/gaudi: 8
                  memory: 400Gi
                  hugepages-2Mi: 95000Mi
              volumeMounts:
                - name: datasets
                  mountPath: /storage
          volumes:
            - name: datasets
              persistentVolumeClaim:
                claimName: shared-model
                readOnly: false
